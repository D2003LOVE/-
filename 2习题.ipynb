{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节课对数据输入及转化介绍\n",
    "\n",
    "# 1. CSV, SQL, PQRQUEL, JSON, XML, HTML, Excel, 等数据格式的输入及转化\n",
    "\n",
    "# 2. 对图片数据进行输入及转化\n",
    "\n",
    "# 3. 对文本数据进行输入及转化\n",
    "\n",
    "# 4. 对时间序列数据进行输入及转化\n",
    "\n",
    "\n",
    "# 一、复习\n",
    "\n",
    "1.torch矢量生成及修改维度\n",
    "练习：\n",
    "1）使用标准正态分布随机生成一个$2\\times 3\\times 4$的张量，并修改其维度为$3\\times 8$。\n",
    "2）使用均匀分布随机生成一个$2\\times 3\\times 4$的张量，并修改其维度为$3\\times 8$。\n",
    "3）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请修改其维度为$2\\times 3\\times 4$。\n",
    "4) 已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请生成一个与其维度一致的全1张量。\n",
    "\n",
    "\n",
    "2.torch张量索引\n",
    "练习：\n",
    "1）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请取出第2行第3列的元素。\n",
    "2）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请取出第2行第3列的元素，并修改其值为100。\n",
    "\n",
    "3. torch张量切片\n",
    "练习：\n",
    "1）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请取出第2行第3列的元素。\n",
    "2）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请取出第2行的元素。\n",
    "\n",
    "4. torch张量拼接\n",
    "练习：\n",
    "1）已知tensor1=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，tensor2=torch.tensor([[[13,14,15,16],[17,18,19,20],[21,22,23,24]]])，请将两个张量拼接在一起。\n",
    "2）已知tensor1=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，tensor2=torch.tensor([[[13,14,15,16],[17,18,19,20],[21,22,23,24]]])，请将两个张量按照行拼接在一起。\n",
    "\n",
    "5. torch张量运算\n",
    "练习：\n",
    "1）已知tensor1=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，tensor2=torch.tensor([[[13,14,15,16],[17,18,19,20],[21,22,23,24]]])，请计算两个张量的和。\n",
    "2）已知tensor1=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，tensor2=torch.tensor([[[13,14,15,16],[17,18,19,20],[21,22,23,24]]])，请计算两个张量的乘积。\n",
    "3）已知tensor1=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，tensor2=torch.tensor([[[13,14,15,16],[17,18,19,20],[21,22,23,24]]])，请计算两个张量的点积。\n",
    "\n",
    "6. torch张量统计\n",
    "练习：\n",
    "1）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请计算张量的均值。\n",
    "2）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请计算张量的最大值。\n",
    "3）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请计算张量的最小值。\n",
    "4）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请计算张量的方差。\n",
    "5）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请计算张量的标准差。\n",
    "\n",
    "7. torch张量排序\n",
    "1）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请对张量进行排序。\n",
    "2）已知tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])，请对张量进行逆序排序。\n",
    "\n",
    "8. torch张量求导    \n",
    "1） 已知张量x=torch.tensor([1,2,3,4,5],requires_grad=True)，请计算x的平方和的导数。\n",
    "2） 已知张量x=torch.tensor([1,2,3,4,5],requires_grad=True)，请计算x的立方和的导数。\n",
    "3） 已知张量y=2x^3+3x^2+4x+5，请计算y对x的导数。\n",
    "4） 已知张量y=2x^3+3x^2+4x+5，请计算y对x的二阶导数。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题作答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch生产标准正态分布\n",
    "def normal(shape):\n",
    "    data = torch.randn(shape)\n",
    "    return data\n",
    "\n",
    "# pytorch生成均匀分布\n",
    "def uniform(shape):\n",
    "    data = torch.rand(shape)\n",
    "    return data\n",
    "\n",
    "# 修改维度\n",
    "def re_shape(data,shape):\n",
    "    return data.view(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9420,  0.0213, -0.7815,  0.6887, -0.8215,  1.4396,  0.6597,  1.2506],\n",
       "        [ 0.8012, -1.0815, -1.3436, -0.6533, -3.0046,  0.4208, -0.3891,  1.2633],\n",
       "        [-0.0866,  0.3465,  0.4803,  1.1210, -1.2786,  0.5436, -1.0626, -1.5050]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1_1.view([3,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([3, 8])\n",
      "\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([3, 8])\n",
      "\n",
      "torch.Size([3, 4])\n",
      "\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "d1_1 = normal([2,3,4])\n",
    "d1_1_ = re_shape(d1_1,[3,8])\n",
    "print(d1_1.size())\n",
    "print(d1_1_.size())\n",
    "print()\n",
    "d1_2 = uniform([2,3,4])\n",
    "d1_2_ = re_shape(d1_2_,[3,8])\n",
    "print(d1_2.size())\n",
    "print(d1_2_.size())\n",
    "print()\n",
    "tensor = torch.tensor([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "tt = tensor.view(3,4)\n",
    "print(tt.size())\n",
    "print()\n",
    "tensor = torch.tensor([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "tt = torch.ones_like(tensor)\n",
    "print(tt.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "tensor([[[  1,   2,   3,   4],\n",
      "         [  5,   6, 100,   8],\n",
      "         [  9,  10,  11,  12]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])\n",
    "print(tensor[0,1,2])\n",
    "\n",
    "tensor[0,1,2]=100\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "第二行： tensor([5, 6, 7, 8])\n",
      "第三列： tensor([ 3,  7, 11])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])\n",
    "print(tensor[0,1,2])\n",
    "tensor = torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])\n",
    "print('第二行：',tensor[0,1,:])\n",
    "print(\"第三列：\",tensor[0,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "按行拼接：\n",
      " tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15, 16],\n",
      "         [17, 18, 19, 20],\n",
      "         [21, 22, 23, 24]]])\n",
      "按列拼接：\n",
      " tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12],\n",
      "         [13, 14, 15, 16],\n",
      "         [17, 18, 19, 20],\n",
      "         [21, 22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "tensor1=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])\n",
    "tensor2=torch.tensor([[[13,14,15,16],[17,18,19,20],[21,22,23,24]]])\n",
    "print(\"按行拼接：\\n\",torch.cat((tensor1,tensor2)))\n",
    "print('按列拼接：\\n',torch.cat((tensor1,tensor2),dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "两个张量的和:\n",
      " tensor([[[14, 16, 18, 20],\n",
      "         [22, 24, 26, 28],\n",
      "         [30, 32, 34, 36]]])\n",
      "两个张量的乘积:\n",
      " tensor([[[ 13,  28,  45,  64],\n",
      "         [ 85, 108, 133, 160],\n",
      "         [189, 220, 253, 288]]])\n",
      "两个张量沿着第 2 维度的点积:\n",
      " tensor([[[[150, 190, 230]],\n",
      "\n",
      "         [[382, 486, 590]],\n",
      "\n",
      "         [[614, 782, 950]]]])\n"
     ]
    }
   ],
   "source": [
    "tensor1=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])\n",
    "tensor2=torch.tensor([[[13,14,15,16],[17,18,19,20],[21,22,23,24]]])\n",
    "print(\"两个张量的和:\\n\",tensor1+tensor2)\n",
    "print(\"两个张量的乘积:\\n\",tensor1*tensor2)\n",
    "print(\"两个张量沿着第 2 维度的点积:\\n\",torch.tensordot(tensor1, tensor2, dims=([2], [2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor1的均值: tensor(6.5000)\n",
      "tensor2的均值: tensor(18.5000)\n",
      "tensor最大值： tensor(12)\n",
      "tensor最小值： tensor(1)\n",
      "tensor方差： tensor(13.)\n",
      "tensor的标准差： tensor(3.6056)\n"
     ]
    }
   ],
   "source": [
    "tensor1=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])\n",
    "tensor2=torch.tensor([[[13,14,15,16],[17,18,19,20],[21,22,23,24]]])\n",
    "print('tensor1的均值:',torch.mean(tensor1.float()))\n",
    "print('tensor2的均值:',torch.mean(tensor2.float()))\n",
    "tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])\n",
    "print('tensor最大值：',torch.max(tensor))\n",
    "print('tensor最小值：',torch.min(tensor))\n",
    "print('tensor方差：',torch.var(tensor.float()))\n",
    "print('tensor的标准差：',torch.std(tensor.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "排序后的tensor：\n",
      " tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]]])\n",
      "按行逆序排序后的tensor：\n",
      " tensor([[[ 4,  3,  2,  1],\n",
      "         [ 8,  7,  6,  5],\n",
      "         [12, 11, 10,  9]]])\n",
      "按列逆序排序后的tensor：\n",
      " tensor([[[ 9, 10, 11, 12],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 1,  2,  3,  4]]])\n"
     ]
    }
   ],
   "source": [
    "tensor=torch.tensor([[[1,2,3,4],[5,6,7,8],[9,10,11,12]]])\n",
    "print('排序后的tensor：\\n',torch.sort(tensor)[0])\n",
    "print('按行逆序排序后的tensor：\\n',torch.sort(tensor,descending=True)[0])\n",
    "print('按列逆序排序后的tensor：\\n',torch.sort(tensor,descending=True,dim=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x的平方和的导数：\n",
      " tensor([ 2.,  4.,  6.,  8., 10.])\n",
      "x的立方和的导数：\n",
      " tensor([ 5., 16., 33., 56., 85.])\n",
      "tensor([ 19.,  40.,  67., 100., 139.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], requires_grad=True)\n",
    "f = torch.sum(x**2)\n",
    "f.backward()\n",
    "print('x的平方和的导数：\\n',x.grad)\n",
    "z = torch.sum(x**3)\n",
    "z.backward()\n",
    "print('x的立方和的导数：\\n',x.grad)\n",
    "y = 2*x**2+3*x**2+4*x+5\n",
    "h = y.sum()\n",
    "h.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
